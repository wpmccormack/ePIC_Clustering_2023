{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40dbd4d3-5334-4a90-9d29-678e382715f2",
   "metadata": {},
   "source": [
    "# Member Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5397ec31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch_geometric.data import DataLoader, Dataset, Data\n",
    "import lightning.pytorch as pl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import itertools\n",
    "import yaml\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import scipy.sparse as sps\n",
    "import xxhash\n",
    "from torch_cluster import knn\n",
    "\n",
    "from epic_clustering.utils import plot_clusters, get_cluster_pos\n",
    "from epic_clustering.models import MemberClassification\n",
    "from epic_clustering.scoring import weighted_v_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ab1ee",
   "metadata": {},
   "source": [
    "## 1. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a4a3c-b17e-4731-bca7-d86f1e858123",
   "metadata": {},
   "source": [
    "Training took me about 2 hours on a single (A100) GPU. If you use a smaller GPU, you may need to decrease the batch size. The configuration I used for this submission is:\n",
    "\n",
    "```\n",
    "input_dir: /global/cfs/cdirs/m3443/data/PowerWeek/train/train/\n",
    "project: PowerWeek_MemberClassification\n",
    "checkpoint_dir: /global/cfs/cdirs/m3443/data/PowerWeek/checkpoints/\n",
    "\n",
    "data_split: [2000, 10, 10]\n",
    "batch_size: 20\n",
    "input_features: 12\n",
    "emb_hidden: 1024\n",
    "nb_layer: 4\n",
    "activation: ReLU\n",
    "\n",
    "warmup: 10\n",
    "lr: 0.01\n",
    "patience: 30\n",
    "max_epochs: 10\n",
    "factor: 0.7\n",
    "num_seeds: 40\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeae4c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to PyG data objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10020 [00:00<?, ?it/s]/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 10020/10020 [00:34<00:00, 290.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 training events, 10 validation events and 10 testing events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"member_classification.yaml\") as f:\n",
    "    member_classification_config = yaml.safe_load(f)\n",
    "model = MemberClassification(member_classification_config)\n",
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd53e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmurnanedaniel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230721_101053-b7a812gl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification/runs/b7a812gl' target=\"_blank\">hearty-music-24</a></strong> to <a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification' target=\"_blank\">https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification/runs/b7a812gl' target=\"_blank\">https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification/runs/b7a812gl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /global/homes/d/danieltm/.conda/envs/powerweek/lib/p ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /global/homes/d/danieltm/.conda/envs/powerweek/lib/p ...\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | network | Sequential | 3.2 M \n",
      "---------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.653    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  69%|██████▉   | 344/500 [03:07<01:25,  1.83it/s, v_num=12gl]"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(project=member_classification_config[\"project\"])\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=300, logger=logger)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c20b94-bff2-4a2e-a7e4-a7bdc74232af",
   "metadata": {},
   "source": [
    "## 2. Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27b1d0-e464-4533-946b-1d6b122c0283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_file = \"/global/cfs/cdirs/m3443/data/PowerWeek/checkpoints/classifier.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50c662-d934-4537-b9a6-c1c049226aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MemberClassification.load_from_checkpoint(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c1f17-3905-4965-89d1-789e78e8ea14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.hparams[\"data_split\"] = [5000, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c9f85-90f4-4863-a8eb-29b7565c2e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369beec-3e7e-4082-a695-f8bc3fac99a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir = \"/global/cfs/cdirs/m3443/data/PowerWeek/train/train\"\n",
    "num_events = sum(model.hparams[\"data_split\"])\n",
    "csv_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')])[:num_events//1000 + 1]\n",
    "events_df = pd.concat([pd.read_csv(f) for f in sorted(csv_files)])\n",
    "if num_events is not None:\n",
    "    events_df = events_df[events_df[\"event\"].isin(sorted(events_df[\"event\"].unique())[:num_events])]\n",
    "events_df['clusterID'] = events_df['clusterID'].astype(np.uint64) # Needed for some reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c8699-01e3-48d4-9203-dc6e455ea4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "def label_hits(event, events_df, pred_edges, max_dist=None):\n",
    "    \n",
    "    seeds_idx = pred_edges.unique()\n",
    "    \n",
    "    event_df = events_df[events_df.event == event.event_id]\n",
    "    \n",
    "    # Collect nonseeds in another tensor\n",
    "    nonseeds_idx = torch.from_numpy(event_df.hit_number[~np.isin(event_df.hit_number.values, seeds_idx.long().numpy())].values).unique()\n",
    "\n",
    "    # For each nonseed find closest seed with knn=1\n",
    "    nonseeds_to_seeds = knn(torch.from_numpy(event_df[np.isin(event_df.hit_number.values, seeds_idx.long().numpy())][['posx', 'posy']].to_numpy()), torch.from_numpy(event_df[np.isin(event_df.hit_number.values, nonseeds_idx.long().numpy())][['posx', 'posy']].to_numpy()), 1)\n",
    "\n",
    "    # Convert 0, .., N indices back to original seed_idx and nonseed_idx\n",
    "    nonseeds_to_seeds = torch.stack([seeds_idx[nonseeds_to_seeds[1]], nonseeds_idx[nonseeds_to_seeds[0]]])\n",
    "    \n",
    "    if max_dist is not None:\n",
    "        positions = torch.from_numpy(events_df[[\"posx\", \"posy\", \"posz\"]].values)\n",
    "        nonseeds_to_seeds = nonseeds_to_seeds[:, torch.sqrt(torch.sum((positions[nonseeds_to_seeds[0]] - positions[nonseeds_to_seeds[1]])**2, dim=-1)) < max_dist]\n",
    "    \n",
    "    # Add the seed-seed edges and the seed-nonseed edges into the same graph\n",
    "    combined_graph = torch.cat([nonseeds_to_seeds, pred_edges], dim=-1)\n",
    "    sparse_edges = to_scipy_sparse_matrix(combined_graph, num_nodes = len(event_df))\n",
    "    \n",
    "    # Perform a connected components algorithm on the graph\n",
    "    _, candidate_labels = sps.csgraph.connected_components(sparse_edges, directed=False, return_labels=True)  \n",
    "    labels = torch.from_numpy(candidate_labels).long()\n",
    "    \n",
    "    event_df['tmp_clusterID'] = labels\n",
    "\n",
    "    # encode the labels to make sure it's unique across all events \n",
    "    str_ids = event_df['event'].astype('str') + \"_\" + event_df['tmp_clusterID'].astype('str')\n",
    "    event_df['labelID'] = [xxhash.xxh64_intdigest(x, seed=0) for x in str_ids.values]\n",
    "    \n",
    "    return event_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59280fe0-5adb-4834-9908-ddfe244dd523",
   "metadata": {},
   "source": [
    "Let's test on the training data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d9818-c263-45f2-82c3-735ccefd5f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelled_events_df = []\n",
    "for event in tqdm(model.trainset):\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            edge_scores = model.cuda()(event.x.cuda()).cpu().squeeze()\n",
    "        labelled_events_df.append(label_hits(event, events_df, event.edge_index[:, edge_scores > 0.6]))\n",
    "    except:\n",
    "        pass\n",
    "labelled_events_df = pd.concat(labelled_events_df)\n",
    "print(f\"Vscore: {weighted_v_score(labels_true=labelled_events_df['clusterID'], labels_pred=labelled_events_df['labelID'], labels_weight=labelled_events_df['E'])[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012b230-b23b-4adb-ac6d-dd0d55789eed",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2eaf6-4ec3-47ad-a8ba-f5c83acb425e",
   "metadata": {},
   "source": [
    "Now, to build the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5469093-6e3a-4854-87ac-12490e4d5af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_file = \"/global/cfs/cdirs/m3443/data/PowerWeek/checkpoints/classifier.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5443c2-cd0b-49cf-96d5-0eb6ad798e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MemberClassification.load_from_checkpoint(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0a4ad-422b-4a79-9c48-6b028def0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.hparams[\"data_split\"] = [10000, 0, 0]\n",
    "model.hparams[\"input_dir\"] = \"/global/cfs/cdirs/m3443/data/PowerWeek/test/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadaa1c-2119-4654-9c6e-83a5f470baa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd8fd0-95b6-42ee-9677-fdf41f356a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir = \"/global/cfs/cdirs/m3443/data/PowerWeek/test/test\"\n",
    "num_events = sum(model.hparams[\"data_split\"])\n",
    "csv_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')])[:num_events//1000 + 1]\n",
    "events_df = pd.concat([pd.read_csv(f) for f in tqdm(sorted(csv_files))])\n",
    "if num_events is not None:\n",
    "    events_df = events_df[events_df[\"event\"].isin(sorted(events_df[\"event\"].unique())[:num_events])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2383427-561e-4d16-9741-b4bb557dab18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelled_events_df = []\n",
    "for event in tqdm(model.trainset):\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            edge_scores = model.cuda()(event.x.cuda()).cpu().squeeze()\n",
    "        labelled_events_df.append(label_hits(event, events_df, event.edge_index[:, edge_scores > 0.65]))\n",
    "    except:\n",
    "        print(f\"Error with event {event}\")\n",
    "labelled_events_df = pd.concat(labelled_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcd40d-7df8-4085-929a-b42372a1d8ea",
   "metadata": {},
   "source": [
    "There are some missing rows for some reason! Let's just add them back in with random labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9949254-2682-406e-b2d7-ab03950ee4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_rows = events_df[~events_df.uniqueID.isin(labelled_events_df.uniqueID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9aedb0-d485-4e4e-880b-ee3cab00412f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_rows['labelID'] = np.random.randint(0, 1000000, (len(missing_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db22944-4ffd-4881-8790-467214f1ed1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelled_events_df = pd.concat([labelled_events_df, missing_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eccdcd-8b66-4825-9959-eef3e3887e04",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c4fe9-ea92-443a-a095-90ea61d859ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelled_events_df[\"clusterID\"] = labelled_events_df[\"labelID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8271c-f7c0-46b2-839b-e4d5f7085648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelled_events_df[[\"uniqueID\", \"clusterID\"]].to_parquet(\"membership_classification.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PowerWeek",
   "language": "python",
   "name": "powerweek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
