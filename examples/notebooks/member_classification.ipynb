{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5397ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch_geometric.data import DataLoader, Dataset, Data\n",
    "import lightning.pytorch as pl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import itertools\n",
    "import yaml\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from epic_clustering.utils import plot_clusters, get_cluster_pos\n",
    "from epic_clustering.models import MemberClassification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9176da3",
   "metadata": {},
   "source": [
    "## 1. Load Model and Test Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7452aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/global/cfs/cdirs/m3443/data/PowerWeek/train/train\"\n",
    "csv_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv(csv_files[0])\n",
    "event = events[events[\"event\"] == events[\"event\"].unique()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted event.E - get first 40\n",
    "high_energy_hits = event.sort_values(by=\"E\", ascending=False).iloc[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ff51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torch meshgrid to get all pairs between high_energy_hits.hit_number and event.hit_number\n",
    "pairs = torch.meshgrid(torch.from_numpy(high_energy_hits.hit_number.values), torch.from_numpy(event.hit_number.values))\n",
    "# convert into a 2 x N array\n",
    "pairs = torch.stack(pairs).reshape(2, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The custom default dataset to load CSV events off the disk\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dir, num_events = None, hparams=None, transform=None, pre_transform=None, pre_filter=None, **kwargs):\n",
    "        super().__init__(input_dir, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.input_dir = input_dir\n",
    "        self.hparams = hparams\n",
    "        self.num_events = num_events\n",
    "        self.scales = {\n",
    "                    \"E\": 30.,\n",
    "                    \"T\": 100.,\n",
    "                    \"posx\": 200.,\n",
    "                    \"posy\": 200.,\n",
    "                    \"posz\": 500.,\n",
    "                }\n",
    "        \n",
    "        self.csv_events = self.load_datafiles_in_dir(self.input_dir, self.num_events)\n",
    "\n",
    "        print(\"Converting to PyG data objects\")\n",
    "        self.pyg_events = [self.convert_to_pyg(event[1]) for event in tqdm(self.csv_events)]\n",
    "        \n",
    "    def load_datafiles_in_dir(self, input_dir, num_events):\n",
    "\n",
    "        # Each file is 1000 events, so need to load num_events//1000 + 1 files\n",
    "        csv_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')][:num_events//1000 + 1]\n",
    "        events = pd.concat([pd.read_csv(f) for f in csv_files])\n",
    "        if num_events is not None:\n",
    "            events = events[events.entry < num_events]\n",
    "\n",
    "        self.scale_features(events)\n",
    "\n",
    "        return list(events.groupby('entry'))\n",
    "\n",
    "    def convert_to_pyg(self, event):\n",
    "\n",
    "        # Convert to PyG data object\n",
    "        event = event.reset_index(drop=True)\n",
    "        event = event.drop(columns=['entry'])\n",
    "\n",
    "        data.edge_index = self.create_training_pairs(event)\n",
    "        y = event.clusterID[data.edge_index[0]] == event.clusterID[data.edge_index[1]]\n",
    "        node_features = torch.from_numpy(event[['posx', 'posy', 'posz', 'E']].to_numpy())\n",
    "        edge_features = torch.cat([node_features[data.edge_index[0]], node_features[data.edge_index[1]]], dim=1)\n",
    "\n",
    "        data = Data(\n",
    "                        x = edge_features,\n",
    "                        y = y\n",
    "                    )\n",
    "\n",
    "        data.num_nodes = data.x.shape[0]\n",
    "\n",
    "        return data\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.pyg_events)\n",
    "\n",
    "    def get(self, idx):\n",
    "\n",
    "        return self.pyg_events[idx]\n",
    "\n",
    "    def scale_features(self, event):\n",
    "        \"\"\"\n",
    "        Handle feature scaling for the event\n",
    "        \"\"\"\n",
    "\n",
    "        for feature in self.scales.keys():\n",
    "            event[feature] = event[feature]/self.scales[feature]\n",
    "\n",
    "        return event\n",
    "\n",
    "    def create_training_pairs(self, event):\n",
    "        \"\"\"\n",
    "        Create the true edge list for the event. This is \n",
    "        \"\"\"\n",
    "\n",
    "        # Sorted event.E - get first 40\n",
    "        high_energy_hits = event.sort_values(by=\"E\", ascending=False).iloc[:40]\n",
    "\n",
    "        # use torch meshgrid to get all pairs between high_energy_hits.hit_number and event.hit_number\n",
    "        pairs = torch.meshgrid(torch.from_numpy(high_energy_hits.hit_number.values), torch.from_numpy(event.hit_number.values))\n",
    "        # convert into a 2 x N array\n",
    "        pairs = torch.stack(pairs).reshape(2, -1)\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EventDataset(input_dir, num_events=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train val and test\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c6ab1ee",
   "metadata": {},
   "source": [
    "## 3. Training Loop Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeae4c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to PyG data objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/220 [00:00<?, ?it/s]/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 220/220 [00:02<00:00, 84.63it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 training events, 10 validation events and 10 testing events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"member_classification.yaml\") as f:\n",
    "    member_classification_config = yaml.safe_load(f)\n",
    "model = MemberClassification(member_classification_config)\n",
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbd53e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmurnanedaniel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230719_063114-jvjv3s7c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification/runs/jvjv3s7c' target=\"_blank\">generous-water-1</a></strong> to <a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification' target=\"_blank\">https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification/runs/jvjv3s7c' target=\"_blank\">https://wandb.ai/murnanedaniel/PowerWeek_MemberClassification/runs/jvjv3s7c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /global/homes/d/danieltm/.conda/envs/powerweek/lib/p ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /global/homes/d/danieltm/.conda/envs/powerweek/lib/p ...\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | network | Sequential | 793 K \n",
      "---------------------------------------\n",
      "793 K     Trainable params\n",
      "0         Non-trainable params\n",
      "793 K     Total params\n",
      "3.172     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [00:07<00:00, 27.67it/s, v_num=3s7c]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.conda/envs/powerweek/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('purity', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 200/200 [00:07<00:00, 26.94it/s, v_num=3s7c]"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(project=member_classification_config[\"project\"])\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=100, logger=logger)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d5595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PowerWeek",
   "language": "python",
   "name": "powerweek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
